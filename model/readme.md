# 모델(Model) 단계

## 1. 개요

- 모델(LLM) 단계는 이전 프롬프트 단계에서 구성된 입력을 기반으로, 대규모 언어 모델을 활용하여 응답을 생성하는 과정
  - 이는 RAG 시스템의 핵심적인 부분으로 언어 모델의 능력을 최대한 활용하여 사용자의 질문에 정확하고 자연스러운 답변 생성

- 위키독스 예시 코드를 일부 변형하여 커밋을 다루고 있습니다. 이 자료를 보는 미래의 저나 다른 분들이 좀 더 이해하기 용이할 것으로 기대합니다. 
  - [출처: 테디노트 위키독스 모델(Model)](https://wikidocs.net/233772)  

- 환경 변수 설정 등은 메인 README를 참고하세요.

## 2. LLM의 필요성
- 사용자 의도 이해
  - LLM은 다양한 언어의 구조, 의미를 깊게 이해하고 있으며 이를 바탕으로 복잡한 질문에 답할 수 있음
  - 자연어 이해(NLU), 자연어 생성(NLG) 능력이 결합되어 보다 자연스럽고 유익한 응답 제공 가능

- 문맥적 적응성
  - LLM은 주어진 문맥을 고려하여 응답 생성
    - 사용자의 질문에 더욱 정확하게 대응 가능
  - 사전 학습된 지식 외 사용자가 제공한 정보에 기반한 답변을 문맥을 참고하여 답변 가능

## 3. LLM의 중요성
- LLM 단계는 사용자의 질문에 대한 답변의 질과 자연스러움을 결정짓는 핵심 요소
- 이 단계에서 LLM은 지금까지 모든 데이터와 정보를 종합하여 사용자의 질문에 최적화된 답변 생성
- LLM의 성능은 RAG 시스템의 전체적 성능과 만족도에 직접적으로 영향을 미침
  - RAG 시스템을 사용하는 많은 응용 분야에서 매우 중요한 역할

## 4. 종류
- OPenAI, Anthropic Claude3 Sonnet, 로컬 모델(llama3-8b) 등 활용
- OPenAI를 주로 사용하기에 해당 모델만 학습했습니다.

### OpenAI
- 채팅 전용 LLM 제공
- 모델을 생성할 때 다양한 옵션을 지정할 수 있음
  - `temperature`
    - 샘플링 온도 설정하는 옵션으로 0 ~ 2 사이에서 선택할 수 있음
    - 높은 값(예 : 0.8)은 출력을 더 무작위하게 만들 수 있음
    - 낮은 값(예 : 0.2)는 출력을 더 집중되고 결정론적으로 만듦

  - `max_tokens`
    - 채팅 완성에서 생성할 토큰의 최대 개수 지정
    - 모델이 한 번에 생성할 수 있는 텍스트 길이 제어
  
  - `model_name`
    - 적용 가능한 모델을 선택하는 옵션

## 5. 구현 정보

- 캐싱 : API 호출 횟수를 줄여 비용 절감과 속도 향상

| 목차                                 | 주요 내용                                           | 링크                                                                   |
|:-----------------------------------|:------------------------------------------------|:---------------------------------------------------------------------|
| **캐싱(Cache)**                      | InMemoryCache, SQLiteCache로 API 호출 최적화 | [캐시(Cache)](https://github.com/CheorHyeon/LangGraphTutorial/pull/18) |
